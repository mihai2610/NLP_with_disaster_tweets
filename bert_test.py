# -*- coding: utf-8 -*-
"""bert_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TOA8C8GX1FffCLGAmT6h-KOj3OW2X0nd
"""

import numpy as np
import math
import re
import pandas as pd
import string
import random
import matplotlib.pyplot as plt
import os

import tensorflow as tf
import tensorflow_hub as hub
from tensorflow.keras import layers, Sequential
import bert
from tensorflow.keras import regularizers



train_data_cols = ["id", "keyword", "location", "text"]
train_file_path = "/content/drive/My Drive/ssl/train.csv"
train_label = "target"
train_cols = [train_label] + train_data_cols
test_file_path = "/content/drive/My Drive/ssl/test.csv"
test_cols = train_data_cols

train_df = pd.read_csv(train_file_path, usecols=train_cols)
test_df = pd.read_csv(test_file_path, usecols=test_cols)


def data_clean(text: str):
	text = re.sub('<[^<]+?>', ' ', text)

	text = text.replace('\\"', '')

	text = text.replace('\n', ' ')

	text = text.replace('\t', ' ')

	text = text.replace('"', '')

	text = text.translate(str.maketrans('', '', string.punctuation))

	text = re.sub(' +', ' ', text)

	text = re.sub('\d+', '0', text)

	text = text.lower()

	return text

test_sentences = [w for w in test_df['text'].values]
train_sentences = [w for w in train_df['text'].values]

data_labels = train_df[train_label].values

bert_layer = hub.KerasLayer("https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1", trainable=False)
vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()
do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()

tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file, do_lower_case)

def encode_sentence(sentence):
	return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence))

train_data_inputs = [encode_sentence(sentence) for sentence in train_sentences]
test_data_inputs = [encode_sentence(sentence) for sentence in test_sentences]

data_with_len = [[sent, data_labels[i], len(sent)] for i, sent in enumerate(train_data_inputs)]
random.shuffle(data_with_len)
data_with_len.sort(key=lambda x: x[2] )
train_sorted_all = [(sent_lab[0], sent_lab[1]) for sent_lab in data_with_len if sent_lab[2] > 2]

train_all_dataset = tf.data.Dataset.from_generator(lambda: train_sorted_all, output_types=(tf.int32, tf.int32))

test_data_all = [sent for i, sent in enumerate(test_data_inputs)]

test_all_dataset = tf.data.Dataset.from_generator(lambda: test_data_all, output_types=(tf.int32))

BATCH_SIZE = 32
test_all_batched = test_all_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None,)))

BATCH_SIZE = 32
train_all_batched = train_all_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None,), ()))


NB_BATCHES = math.ceil(len(train_sorted_all)/BATCH_SIZE)
NB_BATCHES_TEST = NB_BATCHES // 10

train_all_batched.shuffle(NB_BATCHES)
validation_dataset = train_all_batched.take(NB_BATCHES_TEST)
train_dataset = train_all_batched.skip(NB_BATCHES_TEST)

class DCNN(tf.keras.Model):
	def __init__(self, vocab_size, emb_dim=128, nb_filters=50, FFN_units=512, nb_classes=2, dropout_rate=0.1, training=False, name="dcnn"):
		super(DCNN, self).__init__(name=name)

		self.embedding = layers.Embedding(vocab_size, emb_dim)
		self.bigram = layers.Conv1D(filters=nb_filters, kernel_size=2, padding="valid", activation="relu")
		self.trigram = layers.Conv1D(filters=128, kernel_size=3, padding="valid", activation="relu")
		self.fourgram = layers.Conv1D(filters=128, kernel_size=4, padding="valid", activation="relu")
		self.pool = layers.GlobalMaxPooling1D()
		self.dense_1 = layers.Dense(units=FFN_units, activation="tanh")
		self.dropout = layers.Dropout(rate=dropout_rate)
		self.lstm = layers.LSTM(64, kernel_regularizer=regularizers.l2(0.05))

		if nb_classes is 2:
			self.last_dense = layers.Dense(1, activation="sigmoid")
		else:
			self.last_dense = layers.Dense(nb_classes, activation="softmax")

	def call(self, inputs, training):
		x = self.embedding(inputs)
		x_1 = self.bigram(x)
		x_1 = self.pool(x_1)
		x_1 = self.dropout(x_1, training)
		x_2 = self.trigram(x)
		x_2 = self.pool(x_2)
		x_2 = self.dropout(x_2, training)
		x_3 = self.fourgram(x)
		x_3 = self.pool(x_3)
		x_4 = self.lstm(x)
		x_4 = self.dropout(x_4, training)

		merged = tf.concat([x_1, x_2, x_3, x_4], axis=-1)

		merged = self.dense_1(merged)
		merged = self.dropout(merged, training)
		output = self.last_dense(merged)

		return output

VOCAB_SIZE=len(tokenizer.vocab)
EMB_DIM = 300
NB_FILTERS = 256
FNN_units = 64
NB_CLASSES = 2

DROPOUT_RATE = 0.5

NB_EPOCHS = 3

def plot_model_history(model_history):
	fig, axs = plt.subplots(1, 2, figsize=(15, 5))
	# summarize history for accuracy
	axs[0].plot(range(1, len(model_history.history['accuracy']) + 1), model_history.history['accuracy'])
	axs[0].set_title('Model Accuracy')
	axs[0].set_ylabel('Accuracy')
	axs[0].set_xlabel('Epoch')
	axs[0].set_xticks(np.arange(1, len(model_history.history['accuracy']) + 1), len(model_history.history['accuracy'])/10)
	axs[0].legend(['train', 'val'], loc='best')
	# summarize history for loss
	axs[1].plot(range(1, len(model_history.history['loss']) + 1), model_history.history['loss'])
	axs[1].set_title('Model Loss')
	axs[1].set_ylabel('Loss')
	axs[1].set_xlabel('Epoch')
	axs[1].set_xticks(np.arange(1, len(model_history.history['loss']) + 1), len(model_history.history['loss'])/10)
	axs[1].legend(['train', 'val'], loc='best')
	plt.show()
logs_path = "/content/drive/My Drive/ssl/logs"
logdir = os.path.join(logs_path, "log_file")
tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)

dcnn = DCNN(vocab_size=VOCAB_SIZE, emb_dim=EMB_DIM, nb_filters=NB_FILTERS, FFN_units=FNN_units, nb_classes=NB_CLASSES, dropout_rate=DROPOUT_RATE, training=False, name="dcnn")
# dcnn = get_model(vocab_size=VOCAB_SIZE, emb_dim=EMB_DIM, nb_filters=NB_FILTERS, FFN_units=FNN_units, nb_classes=NB_CLASSES, dropout_rate=DROPOUT_RATE, training=False, name="dcnn")
dcnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

history = dcnn.fit(train_dataset, epochs=NB_EPOCHS, callbacks=[tensorboard_callback]) # MyCustomCallback()

eval = dcnn.evaluate(validation_dataset)

plot_model_history(history)



checkpoint_path = "/content/drive/My Drive/ssl/"
ckpt = tf.train.Checkpoint(Dcnn=dcnn)

ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)
if ckpt_manager.latest_checkpoint:
	ckpt.restore(ckpt_manager.latest_checkpoint)
	print("Latest checkoint restored !")

class MyCustomCallback(tf.keras.callbacks.Callback):
	def on_epoc_end(self, epoch, logs=None):
		ckpt.save()
		print("Checkpoint saved at path {}".format(checkpoint_path))

def get_prediction(sentence):
	tokens = encode_sentence(sentence)
	inputs = tf.expand_dims(tokens,0)

	output = dcnn(inputs, training=False)
	sentiment = math.floor(output*2)

	return sentiment

output_csv_path = "/content/drive/My Drive/ssl/out.csv"

with open(output_csv_path, 'w') as file:
	_str = ','.join(['id', 'target'])
	file.write(_str + '\n')

	for index,row in test_df.iterrows():
		text = row['text']
		if len(text.split(" ")) < 4:
			text += " 0 0 0"

		_id = row['id']
		prediction = get_prediction(text)

		_str = ','.join([str(_id), str(int(prediction))])
		file.write(_str)
		file.write('\n')